{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d55fd7d",
   "metadata": {},
   "source": [
    "# Finance Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142030d8",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Business Question:**\n",
    "“How have major U.S. tech stocks performed and co-moved over the past five years, and what can this tell us about risk, return, and diversification in the sector?”\n",
    "\n",
    "**Data:** \n",
    "Stock price and volume data encapsulate investor sentiment, funamentals, and macro factors. Using daily adjusted close prices allows return-based comparison across companies and over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c122b9",
   "metadata": {},
   "source": [
    "## Data Collection & Description\n",
    "Data will be collected via the yahoo finance library. This data is accurate, up to date, and with no missing values. To evaluate performance, we will be pulling the daily high, low, open, close, and volume of each stock as our core metrics and diving deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1e3ec",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ef62a",
   "metadata": {},
   "source": [
    "### Data Collection & Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20017506",
   "metadata": {},
   "source": [
    "Pull data from all tickers into one table with columns `date, close, high, low, open, volume, ticker`. This formatting gives us all the raw data and allows for easier aggregation of metrics in the future. The time window we are interested in looking at is the last 3 years of stock prices. This is a time frame that captures a majority of cycles/patterns and gives us a balance of metrics we can look at without being too wide or narrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['NVDA', 'PLTR', 'GOOGL'] # Nvidia, Palantir, Google\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=3*365)\n",
    "\n",
    "all_data = []\n",
    "for ticker in tickers:\n",
    "    table = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)\n",
    "    \n",
    "    table = table.reset_index()\n",
    "    \n",
    "    table.columns = ['date', 'close', 'high', 'low', 'open', 'volume']\n",
    "    table['ticker'] = ticker\n",
    "    \n",
    "    all_data.append(table)\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "df = df.sort_values(['ticker', 'date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7914d",
   "metadata": {},
   "source": [
    "Great, we were able to ingest our data without any errors, and it looks like it's in the format that we want. We want to make sure our data is clean and tidy, ensuring no duplicates, nulls, or other kinds of invalid values. Let's do some sanity checks like seeing if there are any rows with an incorrect high or low, or if any price is nonpositive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140393e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print('------------------------------------------------------------------------------')\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['date', 'ticker']).sum()}\")\n",
    "print('------------------------------------------------------------------------------')\n",
    "print('Number of Nulls')\n",
    "print (df.isnull().sum())\n",
    "print('------------------------------------------------------------------------------')\n",
    "# High must be >= Open, Close, and Low\n",
    "invalid_high = df[df['high'] < df[['open', 'close', 'low']].max(axis=1)]\n",
    "\n",
    "# Low must be <= Open, Close, and High\n",
    "invalid_low = df[df['low'] > df[['open', 'close', 'high']].min(axis=1)]\n",
    "\n",
    "print(f\"Rows with impossible 'High' prices: {len(invalid_high)}\")\n",
    "print(f\"Rows with impossible 'Low' prices: {len(invalid_low)}\")\n",
    "\n",
    "# Check if any price is non-positive\n",
    "non_positive_prices = df[\n",
    "    (df['open'] <= 0) | (df['high'] <= 0) | \n",
    "    (df['low'] <= 0) | (df['close'] <= 0)\n",
    "]\n",
    "# Check if volume is negative (zero is checked in step 4)\n",
    "negative_volume = df[df['volume'] < 0]\n",
    "\n",
    "print(f\"Rows with non-positive prices: {len(non_positive_prices)}\")\n",
    "print(f\"Rows with negative volume: {len(negative_volume)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8cff1",
   "metadata": {},
   "source": [
    "Everything looks good, we're good to continue. We'll be paying most attention to Close (adjusted) price and volume to evaluate our metrics. Let's see what we're looking at so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Close Price Statistics by Ticker')\n",
    "print(df.groupby('ticker')['close'].describe()[['mean', 'std', 'min', 'max']])\n",
    "sns.lineplot(df, x='date', y='close', hue='ticker')\n",
    "plt.title('Adjusted Close Price Over Time By Ticker')\n",
    "plt.show()\n",
    "sns.lineplot(df, x='date', y='volume', hue='ticker')\n",
    "plt.title('Volume Over Time By Ticker')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50054ff",
   "metadata": {},
   "source": [
    "Just by looking at these simple figures, we can draw the following insights:\n",
    "- NVDA and PLTR seem to show much higher growth and volatility (fluctuation) than GOOGL\n",
    "- NVDA seems to skyrocket beginning mid-2023 which could be linked to the AI boom and consequent GPU demand\n",
    "- PLTR's price sharply climbed in 2024-2025\n",
    "- GOOGL shows steadier, more moderate growth with periodic dips and recoveries.\n",
    "- All three show a strong upward trend in 2024-2025\n",
    "- NVDA has vastly higher trading volume, peaking above 1 billion shares showing strong liquidity and investor attention\n",
    "- GOOGL has relatively stable, low trading volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd0afc",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657d628",
   "metadata": {},
   "source": [
    "### Measuring Growth & Performance\n",
    "To determine how well these stocks are doing, we'll be looking at the following metrics:\n",
    "- Daily Returns: The % change in close price day to day. This measures the % of your equity that you would gain/lose for the day\n",
    "- Cumulative Returns: tracks how much an investment has grown over time relative to the starting point. If cumulative return = 5, then the stock has grown 5x its value at the initial time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"daily_return\"] = df.groupby('ticker')['close'].pct_change()\n",
    "df_googl = df[df['ticker'] == 'GOOGL']\n",
    "df_nvda = df[df['ticker'] == 'NVDA']\n",
    "df_pltr = df[df['ticker'] == 'PLTR']\n",
    "avg_returns = df.groupby('ticker')['daily_return'].mean()\n",
    "\n",
    "sns.lineplot(data=df_googl, x='date', y='daily_return',)\n",
    "plt.title('GOOGL Daily Returns')\n",
    "plt.ylim(-.3, .3)\n",
    "plt.show()\n",
    "sns.lineplot(data=df_nvda, x='date', y='daily_return', color='orange')\n",
    "plt.title('NVDA Daily Returns')\n",
    "plt.ylim(-.3, .3)\n",
    "plt.show()\n",
    "sns.lineplot(data=df_pltr, x='date', y='daily_return', color='green')\n",
    "plt.title('PLTR Daily Returns')\n",
    "plt.ylim(-.3, .3)\n",
    "plt.show()\n",
    "\n",
    "print('Average Daily Returns by Ticker')\n",
    "print(avg_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5c7cd",
   "metadata": {},
   "source": [
    "To no surprise, PLTR leads with the highest average return, with NVDA and GOOGL following in that order. However, telling by these visuals, PLTR seems to vary the most, while GOOGL seems to be the most stable of the 3. Let's take on another perspective by looking at cumulative returns to see how much these equities have compounded the last 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cumulative_return\"] = (1 + df[\"daily_return\"]).groupby(df[\"ticker\"]).cumprod() - 1\n",
    "sns.lineplot(data=df, x='date', y='cumulative_return', hue='ticker')\n",
    "plt.title('Cumulative Returns By Ticker')\n",
    "plt.show()\n",
    "print('Cumulative Returns at End Date')\n",
    "final_cum_returns = (\n",
    "    df.groupby(\"ticker\")[\"cumulative_return\"]\n",
    "      .last()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"cumulative_return\": \"final_cumulative_return\"})\n",
    ")\n",
    "print(final_cum_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163b3d2",
   "metadata": {},
   "source": [
    "Here are some of the insights we can draw from looking at these cumulative returns:\n",
    "- In 3 Years, the value of GOOGL stock has grown 1.92x, NVDA 14.4x, and PLTR 21.6X\n",
    "- We see the same trend, where GOOGL grows slowly and steadily, while PLTR and NVDA have experienced rapid growth especially in the past 2-3 years, likely as a result of AI and analytics surges.\n",
    "- NVDA and PLTR appear to swing a lot more with sharp peaks and dips, indicating volatility. \n",
    "- We see similar movement across the 3, indicating tech sector correlation\n",
    "\n",
    "However, there are a lot of other factors to consider, and these 2 metrics alone don't inform enough about the potential opportunity in investing in these stocks. We're encountering the concept of risk/volatility which is the uncertainty of reward and potential for loss. In our next step, we will quantify risk to give a better overall evaluation of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a92c26",
   "metadata": {},
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of daily returns as a measure of volatility\n",
    "volatility = df.groupby(\"ticker\")[\"daily_return\"].std()\n",
    "sns.barplot(x=volatility.index, y=volatility.values)\n",
    "plt.title(\"Volatility by Ticker\")\n",
    "plt.ylabel(\"Standard Deviation of Daily Returns\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4ba82",
   "metadata": {},
   "source": [
    "This tells us that on average in the last 3 years, GOOGL's returns fluctuate by around 2% every day, while NVDA fluctuates by 3.5% and PLTR by 4.5%. This tells us that GOOGL reflects a mature, large-cap company with stable cash flows and broader investor confidence, whereas PLTR reacts strongly to earnings, hype, and sentiment shifts.\n",
    "\n",
    "However, Volatility can fluctuate over time so this doesn't give us a very accurate depiction of the spread of returns. So, to understand how \"risky\" these stocks have been and where they are right now, we want to calculate rolling volatilities over windows of 30 days, 90 days, etc.. The diffwerent windows inform risks for different kinds of trading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_vol_30d'] = df.groupby('ticker')['daily_return'].rolling(window=30).std().reset_index(0, drop=True)\n",
    "df['rolling_vol_90d'] = df.groupby('ticker')['daily_return'].rolling(window=90).std().reset_index(0, drop=True)\n",
    "sns.lineplot(data=df, x='date', y='rolling_vol_30d', hue='ticker')\n",
    "plt.title('30-Day Rolling Volatility by Ticker')\n",
    "plt.show()\n",
    "sns.lineplot(data=df, x='date', y='rolling_vol_90d', hue='ticker')\n",
    "plt.title('90-Day Rolling Volatility by Ticker')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c51459",
   "metadata": {},
   "source": [
    "Volatility isn't always a bad thing, sometimes the higher the risk, the higher reward. We want to see if these equities are worth the risk, their Sharpe Ratio in other words (the ratio of average daily return to volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af64d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio = avg_returns / volatility * np.sqrt(252)  # Annualized Sharpe Ratio\n",
    "print('Annualized Sharpe Ratios by Ticker')\n",
    "print(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rolling_volatility_30d\"] = (\n",
    "    df.groupby(\"ticker\")[\"daily_return\"]\n",
    "      .rolling(window=30)\n",
    "      .std()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df[\"ma_50\"] = (\n",
    "    df.groupby(\"ticker\")[\"close\"]\n",
    "      .rolling(window=50)\n",
    "      .mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df[\"ma_200\"] = (\n",
    "    df.groupby(\"ticker\")[\"close\"]\n",
    "      .rolling(window=200)\n",
    "      .mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "summary = (\n",
    "    df.groupby(\"ticker\")\n",
    "      .agg(\n",
    "          avg_daily_return = (\"daily_return\", \"mean\"),\n",
    "          volatility = (\"daily_return\", \"std\"),\n",
    "          cumulative_return = (\"cumulative_return\", \"last\"),\n",
    "          sharpe_ratio = (\"daily_return\", lambda x: np.mean(x) / np.std(x)),\n",
    "          max_drawdown = (\"cumulative_return\", lambda x: (x.cummax() - x).max())\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "print(df.tail())\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
